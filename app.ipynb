{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e09e11b1-0cb2-4a3f-a957-07bde740da16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# app.py\n",
    "import io, json, tempfile\n",
    "from pathlib import Path\n",
    "\n",
    "import streamlit as st\n",
    "\n",
    "from resume_parser import parse_resume_llm, ResumeOut  # type: ignore\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a6ffa4c-9734-4a66-ad0a-33fb8b4eae7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys, importlib, inspect\n",
    "sys.path.insert(0, os.getcwd())   # ensure current dir on path\n",
    "\n",
    "import resume_parser as rp         # import the module object\n",
    "importlib.reload(rp)               # pick up edits\n",
    "\n",
    "st.caption(f\"Welcome LLM Based resume parser application\")\n",
    "st.caption(f\"All parsing runs locally\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c7b4e92-48d6-4e4c-8311-3c422c113ae1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def list_ollama_models() -> list[str]:\n",
    "    try:\n",
    "        import ollama\n",
    "        tags = ollama.list().get(\"models\", [])\n",
    "        return [m.get(\"model\") for m in tags if m.get(\"model\")]\n",
    "    except Exception:\n",
    "        return []\n",
    "\n",
    "st.set_page_config(page_title=\"Resume Parser\", page_icon=\"ðŸ“„\", layout=\"centered\")\n",
    "st.title(\"ðŸ“„ Resume Parser â€” Local LLM\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f956d52c-8455-45f6-9e50-387f325ea979",
   "metadata": {},
   "outputs": [],
   "source": [
    "with st.sidebar:\n",
    "    st.header(\"Settings\")\n",
    "    try:\n",
    "        rp.ensure_ollama()\n",
    "        st.success(\"Ollama is reachable.\")\n",
    "    except Exception as e:\n",
    "        st.error(str(e))\n",
    "        \n",
    "    local_models = list_ollama_models()\n",
    "    default_model = \"llama3.1:8b\"\n",
    "    model = st.selectbox(\n",
    "        \"Ollama model\",\n",
    "        options=([default_model] + [m for m in local_models if m != default_model]) or [default_model],\n",
    "        help=\"Use an Ollama model you have pulled locally.\",\n",
    "    )\n",
    "    st.caption(\"Tip: `ollama pull llama3.1:8b` or `phi3:mini`\")\n",
    "\n",
    "st.markdown(\"Upload one or many resumes. Accepted: PDF, DOCX.\")\n",
    "\n",
    "uploaded = st.file_uploader(\n",
    "    \"Drag and drop files or browse\",\n",
    "    type=[\"pdf\", \"docx\"],\n",
    "    accept_multiple_files=True,\n",
    ")\n",
    "\n",
    "results: list[dict] = []\n",
    "if uploaded:\n",
    "    btn = st.button(\"Parse resumes\", type=\"primary\")\n",
    "    if btn:\n",
    "        prog = st.progress(0, text=\"Parsing...\")\n",
    "        for i, up in enumerate(uploaded, start=1):\n",
    "            suffix = \".pdf\" if up.type == \"application/pdf\" or up.name.lower().endswith(\".pdf\") else \".docx\"\n",
    "            with tempfile.NamedTemporaryFile(delete=False, suffix=suffix) as tmp:\n",
    "                tmp.write(up.read())\n",
    "                tmp_path = Path(tmp.name)\n",
    "\n",
    "            with st.spinner(f\"Parsing {up.name}\"):\n",
    "                import time\n",
    "                t0 = time.time()\n",
    "                out = rp.parse_resume_llm(tmp_path, model=model)  # <â€” pass selected model\n",
    "                latency = round(time.time() - t0, 2)\n",
    "\n",
    "                rec = out.model_dump()\n",
    "                rec[\"file\"] = up.name\n",
    "                rec[\"latency_s\"] = latency     # <â€” show latency\n",
    "                results.append(rec)\n",
    "            prog.progress(i / len(uploaded), text=f\"Parsed {i}/{len(uploaded)}\")\n",
    "\n",
    "        prog.empty()\n",
    "\n",
    "        st.subheader(\"Results\")\n",
    "        for rec in results:\n",
    "            with st.container(border=True):\n",
    "                st.markdown(f\"**File:** {rec.get('file','')}\")\n",
    "                if \"error\" in rec:\n",
    "                    st.error(rec[\"error\"])\n",
    "                    continue\n",
    "                st.markdown(f\"**Name:** {rec.get('name','')}\")\n",
    "                st.markdown(f\"**Email:** {rec.get('email','')}\")\n",
    "                skills = rec.get(\"skills\") or []\n",
    "                if skills:\n",
    "                    st.markdown(f\"**Latency:** {rec.get('latency_s', '?')} s\")\n",
    "                with st.expander(\"Raw JSON\"):\n",
    "                    st.json(rec)\n",
    "\n",
    "        if results:\n",
    "            # JSONL download\n",
    "            buf = io.StringIO()\n",
    "            for r in results:\n",
    "                buf.write(json.dumps(r, ensure_ascii=False) + \"\\n\")\n",
    "            st.download_button(\n",
    "                \"Download JSONL\",\n",
    "                buf.getvalue().encode(\"utf-8\"),\n",
    "                file_name=\"resume_parses.jsonl\",\n",
    "                mime=\"application/json\",\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f7f5269-e5d6-4a4d-8a0d-748fa4cb042f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d791dcc-08a0-4dab-ab92-0c298af5bf12",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (llm-env)",
   "language": "python",
   "name": "llm-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
